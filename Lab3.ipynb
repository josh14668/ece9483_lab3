{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NYU ECE-GY 9483 Spring 2025: Lab3 Knowledge Distillation with Quantization and Deployment on Raspberry Pi**\n",
        "\n",
        "### Name: Joshua Manogaran\n",
        "### Netid: jm10765"
      ],
      "metadata": {
        "id": "IyHLmEUw69Q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n"
      ],
      "metadata": {
        "id": "yO-g-LA46-4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we will explore the concept of **knowledge distillation** in deep learning, which involves transferring the knowledge from a large, complex teacher model to a smaller, more efficient student model. Our primary goals are to apply **quantization techniques**—both integer and floating-point—to optimize the student model, and ultimately deploy it on a **Raspberry Pi** for real-world applications.\n",
        "\n",
        "### Objectives:\n",
        "- **Understand Knowledge Distillation:** Learn how a teacher model can guide a student model to achieve comparable performance with fewer parameters.\n",
        "- **Apply Quantization:** Implement both integer and floating-point quantization to reduce model size and improve inference efficiency.\n",
        "- **Deploy on Raspberry Pi:** Prepare and deploy the quantized model on a Raspberry Pi, making it suitable for edge computing.\n",
        "- **Integrate Learning Resources:** Access additional readings and resources for a deeper understanding of the topics.\n",
        "\n",
        "### Lab Outline:\n",
        "1. **Introduction and Objectives:** Overview of the lab, key concepts, and goals. *(This section)*\n",
        "2. **Teacher-Student Model Setup:** Detailed explanation and integration of the pre-developed teacher and student models.  \n",
        "   *(Your custom code for model definitions will be incorporated in this section.)*\n",
        "3. **Knowledge Distillation Process:** Step-by-step guide on transferring knowledge from the teacher to the student model.\n",
        "4. **Quantization Techniques:** Applying integer and floating-point quantization methods to the student model.\n",
        "5. **Deployment on Raspberry Pi:** Instructions on setting up the Raspberry Pi environment, including necessary software installations and configuration.\n",
        "6. **Evaluation and Results:** Analyze the performance improvements and efficiency gains after applying quantization and deployment.\n",
        "7. **Additional Resources:** A curated list of readings and online resources for further study.\n",
        "\n",
        "\n",
        "***Please upload your ipynb file for Lab 3 to Brightspace. Name the file as \"YourNetid_lab_3.ipynb\" and ensure it is submitted by April 3th at 11:59 PM.***"
      ],
      "metadata": {
        "id": "t4MXA27r8nwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommended Readings and Resources:\n"
      ],
      "metadata": {
        "id": "TSWiLe8-7UGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For additional background and deeper insights into the techniques we’ll use, consider reviewing these online resources:\n",
        "- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
        "- [Quantization of Deep Neural Networks for Efficient Inference](https://arxiv.org/abs/1712.05877)\n",
        "- [TensorFlow Lite Model Optimization](https://www.tensorflow.org/lite/performance/model_optimization)\n",
        "- [Getting Started with Raspberry Pi for Machine Learning](https://www.raspberrypi.org/documentation/usage/gpio/)[link text](https://)"
      ],
      "metadata": {
        "id": "0av2dCmb8rva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizing Our Lab**\n",
        "\n",
        "The following diagram illustrates the structured flow of our **Knowledge Distillation and Quantization** process:\n",
        "\n",
        "![Workflow Diagram](https://drive.google.com/uc?id=1JbrMv9g4oA2V2PYI46MjitLRR2DeJK2U)\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Train Teacher Model (ResNet-18)**\n",
        "We begin by training a **ResNet-18** model, which serves as the **teacher model**. It is trained on the dataset to learn complex patterns and generate **soft labels** for knowledge distillation.\n",
        "\n",
        "### **2. Extract Soft Labels (Knowledge Distillation)**\n",
        "Instead of using standard hard labels, we extract **soft labels** from the teacher model. These soft labels contain richer information, allowing the student model to learn **more effectively**.\n",
        "\n",
        "### **3. Define Student Model (MobileNetV2)**\n",
        "A **lightweight student model (MobileNetV2)** is defined. This model will replicate the knowledge of the teacher model but with **fewer parameters** and improved efficiency.\n",
        "\n",
        "### **4. Train Student Model using KD**\n",
        "The student model is trained using **Knowledge Distillation (KD)**, where it learns from both:\n",
        "   - The **original dataset** (hard labels).\n",
        "   - The **soft labels** produced by the teacher model.\n",
        "\n",
        "### **5. Apply Quantization**\n",
        "To optimize the student model for edge deployment, we apply two different types of **quantization**:\n",
        "   - **Float-16 Quantization**: Reduces model size while maintaining floating-point precision.\n",
        "   - **INT-8 Quantization**: Further compresses the model to improve efficiency on low-power devices.\n",
        "\n",
        "### **6. Deploy Quantized Models on Raspberry Pi**\n",
        "The quantized models are deployed on the **Raspberry Pi**, where each version (Float-16 and INT-8) is tested for performance.\n",
        "\n",
        "### **7. Run Inference on Raspberry Pi**\n",
        "Finally, the Raspberry Pi executes real-time inference using the quantized models, allowing efficient **low-latency predictions** in an embedded environment.\n"
      ],
      "metadata": {
        "id": "IpMTplCEGmoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Initialization**\n",
        "\n",
        "In this section, we install and import all required dependencies. This cell sets up the environment to ensure that the necessary packages are available for the lab. Since we are using Google Colab, these commands will automatically install missing libraries.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dx6kmggQ8__o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "9oHF-fG49epW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Knowledge Distillation : Teacher Model**"
      ],
      "metadata": {
        "id": "D6oHoFVJCt-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are teacher models ?\n"
      ],
      "metadata": {
        "id": "x_BI3WZtDE7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teacher models are typically large, high-capacity neural networks that achieve high accuracy on complex datasets. In knowledge distillation, the teacher model serves as a reference from which a smaller, more efficient student model learns. Although the teacher model might be computationally heavy, its robust performance makes it ideal for guiding the training of the student model. In this lab, while we will eventually work with a ResNet‑18 architecture for the teacher model, here’s a sample implementation using a simpler Convolutional Neural Network (CNN) to illustrate the overall approach."
      ],
      "metadata": {
        "id": "4HMrUq9_DVcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Teacher Model Implementation (Simple CNN)"
      ],
      "metadata": {
        "id": "tDBAVE9nDb95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the simple teacher model\n",
        "def simple_teacher_model(input_shape=(32, 32, 3), num_classes=10):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=input_shape),\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create and compile the teacher model\n",
        "teacher_model = simple_teacher_model()\n",
        "teacher_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Load and normalize CIFAR‑10 data\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Train the teacher model\n",
        "history = teacher_model.fit(train_images, train_labels,\n",
        "                            epochs=5,\n",
        "                            batch_size=64,\n",
        "                            validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_acc = teacher_model.evaluate(test_images, test_labels, verbose=2)\n",
        "print(\"Test accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "id": "SVZGLgLq7sJE",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8c31e77-0df7-458c-9c67-e3ab566ba91c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.4117 - loss: 1.6359 - val_accuracy: 0.6112 - val_loss: 1.1178\n",
            "Epoch 2/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6262 - loss: 1.0648 - val_accuracy: 0.6545 - val_loss: 1.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6833 - loss: 0.9073 - val_accuracy: 0.6699 - val_loss: 0.9629\n",
            "Epoch 4/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7214 - loss: 0.8007 - val_accuracy: 0.6777 - val_loss: 0.9350\n",
            "Epoch 5/5\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7551 - loss: 0.7059 - val_accuracy: 0.7037 - val_loss: 0.8961\n",
            "313/313 - 1s - 4ms/step - accuracy: 0.7037 - loss: 0.8961\n",
            "Test accuracy: 0.7037000060081482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the Sample Teacher Model Performance\n",
        "\n",
        "Our simple CNN teacher model achieved about **65% - 70% accuracy** on CIFAR‑10. For effective knowledge distillation, it's crucial to have a high-performing teacher because:\n",
        "\n",
        "- **Enhanced Guidance:** A more accurate teacher provides richer and more reliable soft targets for the student.\n",
        "- **Better Representations:** High accuracy suggests that the teacher has learned robust feature representations, which are vital for the student to mimic.\n",
        "- **Performance Benchmark:** A significant performance gap between teacher and student drives the student to learn and improve.\n",
        "\n",
        "In our next steps, we will work on refining the teacher model (using architectures like ResNet‑18) to ensure it sets a strong foundation for the student model.\n"
      ],
      "metadata": {
        "id": "1IYwsP1ZFmpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. **Teacher Model Implementation – ResNet‑18 ( 20 Marks )**\n",
        "### **You need to achieve accuracy of at least 90 percent for the Teacher Model**"
      ],
      "metadata": {
        "id": "Ijc_HWIwFwc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided skeleton is based on the ResNet‑18 architecture, which is a popular and effective model for image classification tasks. This model is known for its residual connections, which help alleviate the vanishing gradient problem and allow for the training of deeper networks. You can implement the skeleton as provided to achieve an accuracy of at least 85% on CIFAR‑10. Alternatively, if you prefer, you may choose to use another teacher model of your liking—as long as you ensure that its performance is high and that you save it in the same format (i.e., as teacher_model.keras).\n",
        "\n"
      ],
      "metadata": {
        "id": "wnTyRp2pCyWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer:** Below, a skeleton is provided for you. You can implement it as is to achieve an accuracy of at least 90%, or you may choose any other teacher model of your preference and save it as `teacher_model.keras`. Achieving good accuracy is crucial for effective knowledge distillation.\n"
      ],
      "metadata": {
        "id": "2BfZZ8aRAyKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Section 1 - 5 marks***"
      ],
      "metadata": {
        "id": "78c1MUFUyUCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# Section 1: Residual Block\n",
        "##############################################\n",
        "\n",
        "def resnet_block(inputs, filters, strides=1):\n",
        "    x = layers.Conv2D(filters, kernel_size=3, strides=strides, padding='same', use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # TODO: Add another convolution layer\n",
        "    x = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "    # TODO: Add batch normalization\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # TODO: Handle shortcut connection if shape changes\n",
        "    shortcut = inputs\n",
        "    if inputs.shape[-1] != filters or strides != 1:\n",
        "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=strides, use_bias=False)(inputs)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    # TODO: Merge shortcut and main path\n",
        "    x = layers.Add()([x, shortcut])\n",
        "\n",
        "    # TODO: Apply activation\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "6hqcpPLdA4Iz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Section 2 - 10 marks***"
      ],
      "metadata": {
        "id": "lRimVampyatV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# Section 2: ResNet‑18 Model\n",
        "##############################################\n",
        "\n",
        "def ResNet18(input_shape=(32, 32, 3), num_classes=10):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same', use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # TODO: Add residual blocks for Group 1\n",
        "    x = resnet_block(x, filters=64, strides=1)\n",
        "    x = resnet_block(x, filters=64, strides=1)\n",
        "\n",
        "    # TODO: Add residual blocks for Group 2\n",
        "    x = resnet_block(x, filters=128, strides=2)\n",
        "    x = resnet_block(x, filters=128, strides=1)\n",
        "\n",
        "    # TODO: Add residual blocks for Group 3\n",
        "    x = resnet_block(x, filters=256, strides=2)\n",
        "    x = resnet_block(x, filters=256, strides=1)\n",
        "\n",
        "    # TODO: Add residual blocks for Group 4\n",
        "    x = resnet_block(x, filters=512, strides=2)\n",
        "    x = resnet_block(x, filters=512, strides=1)\n",
        "\n",
        "    # TODO: Add global average pooling\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # TODO: Add final dense layer\n",
        "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "cyzWYxcjA67v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Section 3 - 5 marks***"
      ],
      "metadata": {
        "id": "MJN4k0Fgyj1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "##############################################\n",
        "# Section 3: Compile and Train the Teacher Model\n",
        "##############################################\n",
        "\n",
        "# TODO: Create the model\n",
        "teacher_model = ResNet18()\n",
        "\n",
        "# TODO: Compile with a suitable optimizer and loss function\n",
        "teacher_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "# Provided: Load and normalize CIFAR‑10 data\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Provided: Set up data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1\n",
        ")\n",
        "\n",
        "datagen.fit(train_images)\n",
        "\n",
        "# TODO: Train your model using the augmented data\n",
        "history = teacher_model.fit(\n",
        "    datagen.flow(train_images, train_labels, batch_size=64),\n",
        "    epochs=10,\n",
        "    validation_data=(test_images, test_labels)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "RroMfuf4BDfj",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8aa197d-9642-415e-801b-83b9bccfe37b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 54ms/step - accuracy: 0.4019 - loss: 1.6900 - val_accuracy: 0.5237 - val_loss: 1.5650\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.6498 - loss: 0.9907 - val_accuracy: 0.4779 - val_loss: 1.9681\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.7282 - loss: 0.7771 - val_accuracy: 0.5802 - val_loss: 1.4400\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.7659 - loss: 0.6677 - val_accuracy: 0.7445 - val_loss: 0.8039\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - accuracy: 0.8003 - loss: 0.5747 - val_accuracy: 0.7814 - val_loss: 0.6438\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.8177 - loss: 0.5299 - val_accuracy: 0.8231 - val_loss: 0.5325\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.8379 - loss: 0.4719 - val_accuracy: 0.7788 - val_loss: 0.7417\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.8470 - loss: 0.4381 - val_accuracy: 0.8150 - val_loss: 0.5762\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.8613 - loss: 0.3988 - val_accuracy: 0.8080 - val_loss: 0.5980\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - accuracy: 0.8722 - loss: 0.3689 - val_accuracy: 0.8411 - val_loss: 0.4876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Section 4 - Provided***"
      ],
      "metadata": {
        "id": "ow0P3cbiy8jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# Section 4: Save the Trained Model\n",
        "##############################################\n",
        "\n",
        "# Save the trained teacher model\n",
        "teacher_model.save('teacher_model.keras')\n",
        "print(\"Teacher Model Trained and Saved.\")"
      ],
      "metadata": {
        "id": "lP-W-c1vBS14",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfe9136-98e9-4420-942c-ed1b6fbae385"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model Trained and Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. **Knowledge Distillation Process**\n",
        "\n",
        "In this section, you'll implement the teacher–student training loop using knowledge distillation. First, review the complete example below, which demonstrates how to define a custom Distiller class, and loss function. Then, you'll see a skeleton version with TODOs where you are required to fill in some key parts.\n"
      ],
      "metadata": {
        "id": "w9BC6Kh1EZhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Knowledge Distillation: An Overview**\n",
        "\n",
        "## **What is Knowledge Distillation?**\n",
        "Knowledge distillation is a model compression technique where a **large, complex model (teacher)** transfers its learned knowledge to a **smaller, more efficient model (student)**. Instead of training the student model solely on ground-truth labels, distillation incorporates **soft labels**—the probability distributions output by the teacher. These soft labels provide additional information about the relationships between classes, making the student model more generalizable.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. The Distillation Loss Function**\n",
        "\n",
        "To effectively transfer knowledge, the student model is trained using a **distillation loss** that minimizes the difference between the teacher’s and student’s probability distributions. This is typically measured using **Kullback-Leibler (KL) divergence**.\n",
        "\n",
        "### **Mathematical Formulation**\n",
        "Let:\n",
        "- $ z_i^T $ and $ z_i^S $ be the logits (pre-softmax outputs) from the teacher and student models for class $ i $.\n",
        "\n",
        "We introduce a **temperature factor** $ T $, which smooths the probability distributions:\n",
        "\n",
        "$$\n",
        "\\tilde{z}_i = z_i \\Big/ T\n",
        "$$\n",
        "\n",
        "### **Softmax and Log-Softmax**\n",
        "\n",
        "The teacher’s and student’s softened probability distributions are obtained using the **softmax function**:\n",
        "\n",
        "$$\n",
        "p_i^{(T)} = e^{\\tilde{z}_i^T} \\Big/ \\sum_{j} e^{\\tilde{z}_j^T}\n",
        "$$\n",
        "\n",
        "$$\n",
        "q_i^{(T)} = e^{\\tilde{z}_i^S} \\Big/ \\sum_{j} e^{\\tilde{z}_j^S}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ p_i^{(T)} $ represents the teacher’s softened probability distribution.\n",
        "- $ q_i^{(T)} $ represents the student’s softened probability distribution.\n",
        "\n",
        "### **KL Divergence Calculation**\n",
        "The **KL divergence** measures the difference between these distributions:\n",
        "\n",
        "$$\n",
        "D_{KL}(P^{(T)} || Q^{(T)}) = \\sum_{i} p_i^{(T)} \\log \\Big( p_i^{(T)} \\Big/ q_i^{(T)} \\Big)\n",
        "$$\n",
        "\n",
        "### **Final Distillation Loss**\n",
        "The final **distillation loss** is:\n",
        "\n",
        "$$\n",
        "L_{\\text{distill}} = T^2 \\cdot D_{KL}(P^{(T)} || Q^{(T)})\n",
        "$$\n",
        "\n",
        "where the factor $ T^2 $ ensures that the gradients remain properly scaled.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Structure of the Distillation Process**\n",
        "\n",
        "A knowledge distillation framework typically consists of two main components:\n",
        "\n",
        "### **2.1 Teacher Model**\n",
        "- A **pretrained, larger model** that has already been trained on the dataset.\n",
        "- Provides **soft probability distributions** as supervision for the student.\n",
        "- Remains **frozen** during student training.\n",
        "\n",
        "### **2.2 Student Model**\n",
        "- A **smaller, lightweight model** designed to approximate the teacher’s behavior.\n",
        "- Learns using a combination of **hard loss (ground truth labels)** and **soft loss (distillation loss).**\n",
        "\n",
        "The total loss function used to train the student is a **weighted sum** of these two losses:\n",
        "\n",
        "$$\n",
        "L_{\\text{total}} = \\alpha L_{\\text{hard}} + (1 - \\alpha) L_{\\text{distill}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ \\alpha $ balances the contribution of the **hard loss** (standard classification loss) and **distillation loss**.\n",
        "- $ L_{\\text{hard}} $ is the normal classification loss, such as cross-entropy.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. High-Level Structure of a Distillation Framework**\n",
        "The typical implementation follows this structure:\n",
        "\n",
        "### **Step 1: Compute Teacher Predictions**\n",
        "- The teacher model generates probability distributions $ P^{(T)} $.\n",
        "\n",
        "### **Step 2: Compute Student Predictions**\n",
        "- The student model generates probability distributions $ Q^{(T)} $.\n",
        "\n",
        "### **Step 3: Compute Losses**\n",
        "- Compute **hard loss** using standard classification loss.\n",
        "- Compute **soft loss** using KL divergence between teacher and student outputs.\n",
        "\n",
        "### **Step 4: Update Student Model**\n",
        "- Compute gradients with respect to the student model’s parameters.\n",
        "- Optimize the student using a **gradient descent optimizer**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Why Does Distillation Work?**\n",
        "1. **Softer Supervision**  \n",
        "   - The teacher provides additional learning signals through probability distributions, not just hard labels.\n",
        "\n",
        "2. **Regularization Effect**  \n",
        "   - The student learns a **smoother function**, making it more robust to noise.\n",
        "\n",
        "3. **Efficient Compression**  \n",
        "   - The student retains **essential knowledge** from the teacher while reducing model size.\n",
        "\n",
        "---\n",
        "\n",
        " **Try implementing a distillation pipeline based on these principles!**\n"
      ],
      "metadata": {
        "id": "q9X309avHRfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3. **Working on our own Student Model, Distiller Class and Loss function (30 marks)**\n",
        "\n",
        "> ### **Make sure you acheive 75% accuracy for student model after distillation**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gmrD7-AUIL-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Load the Teacher Model\n",
        "\n",
        "Make sure you have downloaded the `teacher_model_resnet18.keras` file from Colab and placed it in your working directory. In this section, we load the teacher model and freeze its weights because we won’t train it further.\n"
      ],
      "metadata": {
        "id": "6Zz2OG08IlZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# 1) LOAD TEACHER MODEL FROM FILE   #\n",
        "#####################################\n",
        "\n",
        "\n",
        "#Make sure you have \"teacher_model.keras\" in your working directory.\n",
        "teacher_model = tf.keras.models.load_model(\"teacher_model.keras\")\n",
        "teacher_model.trainable = False  # Freeze the teacher model\n"
      ],
      "metadata": {
        "id": "hCLluskcIk_Y"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Create the Student Model\n",
        "\n",
        "Here, we define a lightweight student model using MobileNetV2. You can tweak this architecture or use another model if you prefer. The student model will be trained using knowledge distillation.\n"
      ],
      "metadata": {
        "id": "rsMI4ROsIpQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# 2) CREATE A STUDENT MODEL         #\n",
        "#####################################\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "\n",
        "def create_student_model():\n",
        "    \"\"\"\n",
        "    Create a lightweight student model based on MobileNetV2.\n",
        "    You can modify or extend this architecture as needed.\n",
        "    \"\"\"\n",
        "    # Use MobileNetV2 as the base model without pre-trained weights.\n",
        "    base_model = MobileNetV2(weights=None, include_top=False, input_shape=(32, 32, 3))\n",
        "    x = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    # modify or add extra layers.\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "student_model = create_student_model()\n"
      ],
      "metadata": {
        "id": "CNrj2MjTIrSC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Define the Distiller Class (20 marks)\n",
        "\n",
        "The distiller class handles the custom training loop for knowledge distillation. It calculates a combined loss from:\n",
        "- **Hard Loss:** The standard loss between the student's predictions and the true labels.\n",
        "- **Soft Loss:** The distillation loss between the teacher’s and student's predictions (using KL divergence).\n",
        "\n",
        "Complete the TODOs to implement the missing parts.\n"
      ],
      "metadata": {
        "id": "179SgQbQI0rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# 3) DEFINE THE DISTILLER CLASS     #\n",
        "#####################################\n",
        "class Distiller(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Handles the custom training loop for knowledge distillation:\n",
        "    - Uses teacher predictions (soft targets).\n",
        "    - Uses student predictions.\n",
        "    - Calculates a combined loss (hard + soft).\n",
        "    \"\"\"\n",
        "    def __init__(self, student, teacher):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.student = student\n",
        "        self.teacher = teacher\n",
        "\n",
        "    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3):\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Obtain teacher predictions (teacher is frozen)\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # TODO: 1) Compute student predictions for x.\n",
        "            student_predictions = self.student(x, training=True)\n",
        "\n",
        "            # TODO: 2) Compute the \"hard\" loss\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "\n",
        "            # TODO: 3) Compute the \"soft\" loss (distillation loss)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "                self.temperature\n",
        "            )\n",
        "\n",
        "            # TODO: 4) Combine the two losses (hard and soft)\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
        "\n",
        "        # TODO: 5) Compute gradients and update the student model's trainable variables.\n",
        "        gradients = tape.gradient(loss, self.student.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.student.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results[\"loss\"] = loss\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, y = data\n",
        "        # TODO: 1) Compute student predictions in inference mode.\n",
        "        y_pred = self.student(x, training=False)\n",
        "\n",
        "        # TODO: 2) Compute the student loss comparing y_pred to y.\n",
        "        student_loss = self.student_loss_fn(y, y_pred)\n",
        "\n",
        "        # Update metrics\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results[\"loss\"] = student_loss\n",
        "        return results\n",
        "\n"
      ],
      "metadata": {
        "id": "5Jk3lYwOI13o"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Define Loss Functions (10 marks)\n",
        "\n",
        "Define the loss functions used in the distillation process:\n",
        "- **Hard Loss:** Standard loss between student predictions and true labels.\n",
        "- **Distillation Loss:** Uses KL divergence to measure the difference between the softened predictions of the teacher and student.\n"
      ],
      "metadata": {
        "id": "GLSdQtUmI-uQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# 4) DEFINE LOSS FUNCTIONS          #\n",
        "#####################################\n",
        "# Hard label loss (student vs. ground truth)\n",
        "\n",
        "student_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "# Distillation loss function (KL divergence)\n",
        "def distillation_loss_fn(teacher_logits, student_logits, temperature):\n",
        "    \"\"\"\n",
        "    Compute the distillation loss using KL divergence.\n",
        "    \"\"\"\n",
        "    # TODO: Compute teacher probabilities using softmax with temperature scaling.\n",
        "    teacher_probs = tf.nn.softmax(teacher_logits / temperature, axis=1)\n",
        "\n",
        "    # TODO: Compute student log-probabilities using log-softmax with temperature scaling.\n",
        "    student_log_probs = tf.nn.log_softmax(student_logits / temperature, axis=1)\n",
        "\n",
        "    # TODO: Compute the KL divergence loss and return it.\n",
        "    kl = tf.reduce_mean(tf.reduce_sum(teacher_probs * student_log_probs, axis=1))\n",
        "\n",
        "\n",
        "    return kl\n"
      ],
      "metadata": {
        "id": "FzWskfSzJAIT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Compile & Train the Distiller\n",
        "\n",
        "In this final section, we create an instance of the Distiller, compile it, and train it using the CIFAR-10 dataset. Optionally, data augmentation is applied.\n"
      ],
      "metadata": {
        "id": "gHcMNmQvJN_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# 5) COMPILE & TRAIN DISTILLER      #\n",
        "#####################################\n",
        "# Create the Distiller instance.\n",
        "distiller = Distiller(student=student_model, teacher=teacher_model)\n",
        "\n",
        "# Compile the distiller.\n",
        "distiller.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=student_loss_fn,\n",
        "    distillation_loss_fn=distillation_loss_fn,\n",
        "    alpha=0.1,       # Weight for the hard loss\n",
        "    temperature=3    # Temperature for softening teacher predictions\n",
        ")\n",
        "\n",
        "# Load and normalize CIFAR-10 dataset.\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Optional: Data augmentation.\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen.fit(train_images)\n",
        "\n",
        "# TODO: Train the distiller using the data generator.\n",
        "distiller.fit(\n",
        "    datagen.flow(train_images, train_labels, batch_size=64),\n",
        "    epochs=20,\n",
        "    validation_data=(test_images, test_labels)\n",
        ")\n"
      ],
      "metadata": {
        "id": "kuQU2tojJQBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad50cd5-4386-47ba-cf4f-de569d9fb0ff"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:642: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
            "```\n",
            "for metric in self.metrics:\n",
            "    metric.update_state(y, y_pred)\n",
            "```\n",
            "\n",
            "  return self._compiled_metrics_update_state(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 67ms/step - sparse_categorical_accuracy: 0.2222 - loss: -1.8834 - val_loss: 2.3209\n",
            "Epoch 2/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.3947 - loss: -1.9125 - val_loss: 2.3122\n",
            "Epoch 3/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.4545 - loss: -1.9246 - val_loss: 2.3403\n",
            "Epoch 4/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.5025 - loss: -1.9354 - val_loss: 2.3292\n",
            "Epoch 5/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.5323 - loss: -1.9437 - val_loss: 2.3490\n",
            "Epoch 6/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.5659 - loss: -1.9521 - val_loss: 1.8000\n",
            "Epoch 7/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.5942 - loss: -1.9578 - val_loss: 1.7272\n",
            "Epoch 8/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.6102 - loss: -1.9636 - val_loss: 1.7743\n",
            "Epoch 9/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.6388 - loss: -1.9691 - val_loss: 2.1328\n",
            "Epoch 10/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.6497 - loss: -1.9735 - val_loss: 3.4693\n",
            "Epoch 11/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.6699 - loss: -1.9775 - val_loss: 4.2885\n",
            "Epoch 12/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.6859 - loss: -1.9813 - val_loss: 3.7136\n",
            "Epoch 13/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.6896 - loss: -1.9842 - val_loss: 2.1618\n",
            "Epoch 14/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 36ms/step - sparse_categorical_accuracy: 0.7019 - loss: -1.9870 - val_loss: 0.6897\n",
            "Epoch 15/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.7111 - loss: -1.9901 - val_loss: 1.6085\n",
            "Epoch 16/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.7231 - loss: -1.9923 - val_loss: 0.9647\n",
            "Epoch 17/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.7304 - loss: -1.9936 - val_loss: 1.7543\n",
            "Epoch 18/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.7347 - loss: -1.9956 - val_loss: 0.7111\n",
            "Epoch 19/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.7449 - loss: -1.9976 - val_loss: 1.3743\n",
            "Epoch 20/20\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 35ms/step - sparse_categorical_accuracy: 0.7517 - loss: -1.9993 - val_loss: 0.6495\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7dcc3262b410>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "# 6) SAVE STUDENT MODEL & TFLITE    #\n",
        "#####################################\n",
        "\n",
        "# Save the distilled student model\n",
        "student_model.save(\"student_model.keras\")\n",
        "print(\"Student model saved.\")"
      ],
      "metadata": {
        "id": "9swy1S7aKq3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6a918b-2437-4d99-cb3d-1f9d32855c63"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4. **TensorFlow Lite Quantization ( 10 marks )**\n",
        "\n",
        "In this section, we convert the saved student model to TensorFlow Lite format using two different quantization methods:\n",
        "- **Float16 Weight-Only Quantization:** Converts model weights from float32 to float16.\n",
        "- **INT8 Weight-Only Quantization:** Converts model weights to int8 using a representative dataset for calibration while keeping activations in float32.\n",
        "\n",
        "Review the code, then complete the TODO sections with appropriate settings or modifications as needed.\n"
      ],
      "metadata": {
        "id": "C9eI5tdlLa5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Section 1 - 5 marks***"
      ],
      "metadata": {
        "id": "4w7yVDSVzowO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Float16 Weight-Only Quantization\n",
        "# -------------------------------\n",
        "\n",
        "# TODO: 1) Load the student model from a saved Keras model file.\n",
        "student_model = tf.keras.models.load_model(\"student_model.keras\")\n",
        "\n",
        "# TODO: 2) Create a TFLite converter from the student model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(student_model)\n",
        "\n",
        "# TODO: 3) Set the converter optimizations to enable model compression.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# TODO: 4) Define the target data type for quantization (e.g., float16 for weight-only quantization).\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "# TODO: 5) Convert the model to TFLite format.\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted TFLite model to a file.\n",
        "with open('student_model_float16_weights_only.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "ibjrninJMS22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7d5f13b-f6db-4222-f5b1-1db49a8108be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpo48qcg9t'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='input_layer_2')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  138315925065744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925076688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925076304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925076496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925077840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925075152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925074768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925074384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925074576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925077264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925073232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925072848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925072464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925072656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925077072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925071312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925070928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925070544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925070736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925075536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925069392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925069008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925068624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925068816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925073616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925067472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925067088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925066512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925066896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925071696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925068240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922604688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922605264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315925067856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922604112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922606416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922606800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922607184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922606992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922604304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922608336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922608720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922609104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922608912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922604880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922610256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922610640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922611024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922610832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922606032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922612176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922612560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922612944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922612752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922607952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922614096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922614480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922614864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922614672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922609872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922616016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922616400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922616784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922616592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922611792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922617936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922618320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922618704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922618512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922613712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922619856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922604496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922620240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922620048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315922617552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923145168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923146512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923146896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923146704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923145360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923148048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923148432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923148816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923148624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923145936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923149968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923150352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923150736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923150544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923146128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923151888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923152272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923152656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923152464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923147664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923153808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923154192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923154576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923154384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923149584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923155728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923156112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923156496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923156304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923151504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923157648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923158032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923158416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923158224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923153424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923159568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923159952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923160720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923160144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923155344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923158800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923522192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923522768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923159184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923521808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923523920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923524304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923524688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923524496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923521616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923525840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923526224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923526608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923526416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923522384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923527760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923528144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923528528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923528336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923523536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923529680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923530064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923530448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923530256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923525456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923531600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923531984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923532368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923532176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923527376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923533520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923533904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923534288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923534096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923529296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923535440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923535824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923536208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923536016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923531216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923537360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923522000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923537744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923537552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315923535056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924062672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924064016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924064400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924064208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924062864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924065552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924065936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924066320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924066128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924063440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924067472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924067856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924068240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924068048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924063632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924069392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924069776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924070160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924069968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924065168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924071312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924071696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924072080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924071888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924067088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924073232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924073616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924074000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924073808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924069008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924075152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924075536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924075920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924075728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924070928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924077072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924077456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924078224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924077648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924072848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924076304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924227088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924226128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924076688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924226320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924228432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924228816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924229200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924229008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924226512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924230352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924230736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924231120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924230928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924226896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924232272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924232656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924233040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924228048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924234192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924234576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924234960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924234768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924229968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924236112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924236496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924236880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924236688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924231888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924238032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924238416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924238800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924238608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924233808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924239952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924240336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924240720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924240528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924235728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924241872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924227280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924242256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924242064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924239568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924208144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924207760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924207376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924207568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924209488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924206224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924205840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924205456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924205648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924209296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924204304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924203920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924203536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924203728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924208720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924202384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924201808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924202768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924201040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Section 2 - 5 marks***"
      ],
      "metadata": {
        "id": "0di27L4QzuhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# INT8 Weight-Only Quantization\n",
        "# -------------------------------\n",
        "\n",
        "# TODO: 1) Load the student model from a saved Keras model file.\n",
        "student_model = tf.keras.models.load_model(\"student_model.keras\")\n",
        "\n",
        "# TODO: 2) Load the CIFAR-10 test dataset for calibration.\n",
        "(_, _), (test_images, _) = tf.keras.datasets.cifar10.load_data()\n",
        "test_images = test_images / 255.0  # Normalize pixel values\n",
        "\n",
        "# TODO: 3) Define a representative dataset generator for calibration.\n",
        "def representative_data_gen():\n",
        "    # Verify if 100 images are sufficient for calibration or adjust the count.\n",
        "    for i in range(100):\n",
        "        yield [test_images[i:i+1].astype(np.float32)]\n",
        "\n",
        "# TODO: 4) Create a TFLite converter from the student model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(student_model)\n",
        "\n",
        "# TODO: 5) Set the converter optimizations for weight quantization.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# TODO: 6) Assign the representative dataset generator for calibration.\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# TODO: 7) Set the target operations to use only INT8 built-in ops.\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# TODO: 8) Keep activations as float32 for compatibility, unless full INT8 quantization is needed.\n",
        "converter.inference_input_type = tf.float32\n",
        "converter.inference_output_type = tf.float32\n",
        "\n",
        "# TODO: 9) Convert the model to TFLite format.\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted TFLite model to a file.\n",
        "with open('student_model_int8_weights_only_cifar.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "NDo5QvCRMUMg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dede38b-5675-46f9-fcd7-05eb098778ad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpx0w81bz2'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='input_layer_2')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  138315924193936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924195280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924195664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924195472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924194128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924193744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924198160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924198544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924198352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924194512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924199696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924200080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924200464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924200272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924197008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924202000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924196432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924206608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924201232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315924199312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276960400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276947728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276945616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276944656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276945232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276959056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276960208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276951376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276952528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276958480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276955216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200653776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276954640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276954064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276950224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138321276955408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144375952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144376336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200647248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200653584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144377488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144377872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144378256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144378064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144375376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144379408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144379792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144380176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144379984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144375760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144381328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144381712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144382096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144381904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144377104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144383248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144383632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144384016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144383824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144379024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144385168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144385552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144385936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144385744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144380944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144387088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144387472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144387856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144387664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144382864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144389008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144389392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144389776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144389584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144384784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144390928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144391504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144390160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144391312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317144388624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356777744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356779088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356779472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356779280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356778704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356780624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356781008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356781392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356781200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356777936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356782544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356782928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356783312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356783120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356778128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356784464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356784848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356785232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356785040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356780240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356786384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356786768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356787152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356786960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356782160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356788304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356788688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356789072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356788880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356784080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356790224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356790608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356790992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356790800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356786000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356792144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356792528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356792912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356792720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356787920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356791376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356712592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356712016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356793680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356789840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356714128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356714512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356714896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356714704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356712208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356716048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356716432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356716816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356716624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356712784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356717968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356718352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356718736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356718544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356713744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356719888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356720272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356720656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356720464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356715664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356721808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356722192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356722576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356722384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356717584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356723728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356724112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356724496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356724304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356719504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356725648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356726032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356726416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356726224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356721424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356727568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356728144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356726800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356727952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317356725264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357023504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357024848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357025232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357025040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357024464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357026384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357026768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357027152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357026960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357023696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357028304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357028688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357029072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357028880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357023888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357030224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357030608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357030992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357030800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357026000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357032144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357032528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357032912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357032720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357027920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357034064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357034448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357034832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357034640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357029840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357035984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357036368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357036752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357036560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357031760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357037904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357038288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357038672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357038480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357033680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357037136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944673936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944674320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357039440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317357035600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944675472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944675856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944677584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944676048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944673360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944678160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944678544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944677776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944675088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944676624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944679504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944679888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944680272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944680080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944678736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944681424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944681808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944682192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944682000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944678352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944683344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944683728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944684112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944683920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944679120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944685264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944685648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944686032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944685840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944681040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944687184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944687568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944687952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944687760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944682960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944689104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944676816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944689488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944689296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138315944686800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200391632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200390288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200389904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200390096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200391440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200388752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200388368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200387984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200388176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200390864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200386832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200386256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200387216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138317200385488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5. **Raspberry Pi Setup on Mythic Beasts ( 40 mark )**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YUn7R5VCamJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will set up a Raspberry Pi 4 server through [Mythic Beasts](https://www.mythic-beasts.com/order/rpi/). This server will be used to deploy and run our quantized TensorFlow Lite models. Follow the steps below to get started.\n",
        "\n",
        "### 1. Purchase a Raspberry Pi 4 (8 GB) Server ( approx 10 pounds )\n",
        "- Go to [Mythic Beasts Raspberry Pi](https://www.mythic-beasts.com/order/rpi/).\n",
        "- Choose the **Raspberry Pi 4 with 8 GB RAM** monthly plan.\n",
        "- Complete the checkout process to finalize your purchase.\n",
        "\n",
        "### 2. Name Your Server\n",
        "- During setup, you will have the option to name your server.\n",
        "- **Important:** Use the format `netid_lab3` to clearly identify your server for this lab.\n",
        "- Verify that the name is correct before proceeding.\n",
        "\n",
        "### 3. Generate SSH Keys ( Mac )\n",
        "To securely connect to your Raspberry Pi server, you will need SSH keys:\n",
        "\n",
        "- Open a terminal (on macOS/Linux) or use Git Bash/PowerShell (on Windows).\n",
        "- Generate a new SSH key pair (if you don’t already have one) by running:\n",
        "\n",
        "  ```bash\n",
        "  ssh-keygen -t rsa -b 4096\n",
        "  ```\n",
        "Press Enter to accept the default file location or specify a custom path.\n",
        "\n",
        "### 4. Add Your SSH Public Key to Mythic Beasts\n",
        "1. Log in to your **Mythic Beasts** account and navigate to the **SSH Keys** section under **Servers**.\n",
        "2. Click the option to **Add a New Key**.\n",
        "3. Paste the **public** portion of your SSH key (the content of `~/.ssh/id_rsa.pub` or equivalent) into the key field.\n",
        "4. Save the changes.\n",
        "\n",
        "### 5. Connect to Your Raspberry Pi ( Mac )\n",
        "Once your server is set up and the SSH keys are in place:\n",
        "\n",
        "1. Obtain your **Raspberry Pi server’s IP address** from the Mythic Beasts control panel.\n",
        "2. Open a terminal and connect via SSH:\n",
        "\n",
        "   ```bash\n",
        "   ssh -i ~/.ssh/id_rsa <username>@<your_pi_server_ip>\n",
        "\n",
        "3. Replace:\n",
        "~/.ssh/id_rsa with the path to your private key if different.\n",
        "<username> with the username provided by Mythic Beasts (e.g., netid_lab3).\n",
        "<your_pi_server_ip> with the IP address of your Raspberry Pi.\n",
        "\n",
        "These configurations are also provided by mythic beasts"
      ],
      "metadata": {
        "id": "XT9tQTZcfkH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Transferring Files to Raspberry Pi using SCP\n",
        "\n",
        "To transfer files from your local machine to a Raspberry Pi hosted by Mythic Beasts, use the `scp` (Secure Copy Protocol) command.\n",
        "\n",
        "## Command Format\n",
        "\n",
        "```sh\n",
        "scp -i ~/your-key-file -P 5400 \"/path/to/local/file\" root@ssh.your-hostedpi.com:/path/to/destination/\n",
        "```\n",
        "## Example Command\n",
        "``` sh\n",
        "scp -i ~/.ssh/mythic_key -P 5400 \"~/Documents/my_script.py\" root@ssh.examplepi.com:/home/pi/\n",
        "```\n",
        "### Important Instructions:\n",
        "- Ensure that the file transfer completes **100%** before proceeding.\n",
        "- If the `scp` command is interrupted or incomplete, your files may be **corrupted**, leading to runtime errors.\n",
        "- Always **wait for the command to fully execute** before disconnecting.\n",
        "\n",
        "## **NOTE** :\n",
        "For now transfer both the **\"student_model_int8_weights_only_cifar.tflite\"** and **\"student_model_float16_weights_only.tflite\"** to the raspberry pi using SCP command. The above setup was for macos it follows similarly for windows systems as well you can find it online i.e (how to setup scp,ssh etc )."
      ],
      "metadata": {
        "id": "3SfM9VJeiHCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Downloading Images on Raspberrypi to run Inference"
      ],
      "metadata": {
        "id": "Kx-_04a-Kw4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code on raspberry pi"
      ],
      "metadata": {
        "id": "WW48lxNsLAKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mkdir -p images\n",
        "\n",
        "wget -O images/image1_frog.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/frog1.png\n",
        "wget -O images/image2_automobile.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/automobile1.png\n",
        "wget -O images/image3_truck.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/truck1.png\n",
        "wget -O images/image4_airplane.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/airplane1.png\n",
        "wget -O images/image5_ship.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/ship1.png\n",
        "wget -O images/image6_deer.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/deer1.png\n",
        "wget -O images/image7_dog.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/dog1.png\n",
        "wget -O images/image8_horse.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/horse1.png\n",
        "wget -O images/image9_bird.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/bird1.png\n",
        "wget -O images/image10_cat.jpg https://www.cs.toronto.edu/~kriz/cifar-10-sample/cat1.png\n"
      ],
      "metadata": {
        "id": "wDzfVko0jgp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Attach Screenshots of Your Setup Displaying Your NetID (10 Marks)\n",
        "\n",
        "Your submission should include the following screenshots:\n",
        "\n",
        "1. **Server Displaying Your NetID (`netid_lab3`)**  \n",
        "   - Show the terminal where your server is named as `netid_lab3`.\n",
        "\n",
        "2. **SSH Login Screenshot**  \n",
        "   - A terminal screenshot where you are logged into your server via SSH.\n",
        "\n",
        "3. **`wget` Command for Downloading Images**  \n",
        "   - A screenshot showing the execution of the `wget` command to download the required images.\n",
        "\n",
        "4. **SCP Command for Transferring `.tflite` Models to the PO**  \n",
        "   - Use the following command to transfer your TensorFlow Lite models from your local machine to the Pi:  \n",
        "\n"
      ],
      "metadata": {
        "id": "Tq4MS3IKz6Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKIGNOA1054W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Running Inference on Raspberry Pi ( 20 marks )\n",
        "\n",
        "After transferring the **TFLite models** and downloading the **images**, the next step is to **run inference** using the `Inference.py` script.\n",
        "\n",
        "---\n",
        "\n",
        "## **Setup Requirements**\n",
        "Ensure that:\n",
        "- The **quantized models** (`student_model_float16_weights_only.tflite`, `student_model_int8_weights_only_cifar.tflite`) are in the working directory.\n",
        "- The **test images** are inside the `images/` folder.\n",
        "- The required **libraries** (`tflite_runtime`, `PIL`, `numpy`, `psutil`, `shutil`) are installed.\n",
        "\n",
        "---\n",
        "\n",
        "## **Implement & Transfer Inference Script**\n",
        "Now, complete the **TODOs in `Inference.py`** to:\n",
        "- Load the **TFLite models**.\n",
        "- Preprocess images and run inference.\n",
        "- Measure CPU, memory, and temperature usage.\n",
        "\n",
        "Once completed, **transfer `Inference.py` to the Raspberry Pi using SCP** and run inference to analyze performance.\n",
        "\n",
        " **Complete the script, transfer it, and start running inference!**\n"
      ],
      "metadata": {
        "id": "qe-Lc5fBM2Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Section 1 - 10 marks (complete run inference function )***"
      ],
      "metadata": {
        "id": "Ila4LPui1HwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tflite_runtime.interpreter import Interpreter\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import psutil\n",
        "import shutil\n",
        "\n",
        "# List of models to test\n",
        "models = [\n",
        "    \"student_model_float16_weights_only.tflite\",\n",
        "    \"student_model_int8_weights_only_cifar.tflite\"\n",
        "]\n",
        "\n",
        "# Path to images folder\n",
        "image_folder = \"images/\"\n",
        "image_files = sorted([f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png'))])\n",
        "\n",
        "# Ensure images exist\n",
        "if not image_files:\n",
        "    print(\"No images found in the 'images/' folder. Please add images before running inference.\")\n",
        "    exit()\n",
        "\n",
        "# Function to get CPU temperature\n",
        "def get_cpu_temp():\n",
        "    try:\n",
        "        with open(\"/sys/class/thermal/thermal_zone0/temp\", \"r\") as f:\n",
        "            return int(f.read().strip()) / 1000.0  # Convert from millidegrees to Celsius\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Function to preprocess images\n",
        "def preprocess_image(image_path, input_shape, dtype=np.float32):\n",
        "    \"\"\"Load and preprocess an image to match the model's expected input shape.\"\"\"\n",
        "    img = Image.open(image_path).resize((input_shape[1], input_shape[2]))\n",
        "    img_array = np.array(img).astype(dtype) / 255.0  # Normalize\n",
        "\n",
        "    # INT8 models require int8 inputs, so scale correctly\n",
        "    if dtype == np.int8:\n",
        "        img_array = (img_array * 255 - 128).astype(np.int8)\n",
        "\n",
        "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "    return img_array\n",
        "\n",
        "# -------------------------------\n",
        "# Function to run inference\n",
        "# -------------------------------\n",
        "\n",
        "def run_inference(model_path, image_path):\n",
        "    \"\"\"Run inference on a quantized TFLite model and collect performance data.\"\"\"\n",
        "    try:\n",
        "        # TODO: 1) Load the TFLite model using the Interpreter.\n",
        "        interpreter = ...\n",
        "\n",
        "        # TODO: 2) Allocate memory for model tensors.\n",
        "        ...\n",
        "\n",
        "        # TODO: 3) Retrieve model input and output tensor details.\n",
        "        input_details = ...\n",
        "        output_details = ...\n",
        "\n",
        "        # TODO: 4) Extract input shape and data type.\n",
        "        input_shape = ...\n",
        "        input_dtype = ...\n",
        "\n",
        "        # TODO: 5) Load and preprocess the image to match the model’s input format.\n",
        "        sample_input = ...\n",
        "\n",
        "        # TODO: 6) Measure CPU and memory usage before inference.\n",
        "        cpu_before = ...\n",
        "        mem_before = ...\n",
        "        temp_before = ...\n",
        "\n",
        "        # TODO: 7) Perform inference:\n",
        "        #  - Start timer\n",
        "        #  - Set the input tensor\n",
        "        #  - Invoke the model\n",
        "        #  - End timer\n",
        "        start_time = ...\n",
        "        ...\n",
        "        ...\n",
        "        end_time = ...\n",
        "\n",
        "        # TODO: 8) Retrieve the output tensor from the model.\n",
        "        output_data = ...\n",
        "\n",
        "        # TODO: 9) Determine the predicted class from the output.\n",
        "        predicted_class = ...\n",
        "\n",
        "        # TODO: 10) Measure CPU and memory usage after inference.\n",
        "        cpu_after = ...\n",
        "        mem_after = ...\n",
        "        temp_after = ...\n",
        "\n",
        "        return {\n",
        "            \"model\": model_path,\n",
        "            \"image\": os.path.basename(image_path),\n",
        "            \"inference_time\": round(end_time - start_time, 4),\n",
        "            \"predicted_class\": predicted_class,\n",
        "            \"cpu_before\": cpu_before,\n",
        "            \"cpu_after\": cpu_after,\n",
        "            \"mem_before\": mem_before,\n",
        "            \"mem_after\": mem_after,\n",
        "            \"temp_before\": temp_before,\n",
        "            \"temp_after\": temp_after\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Inference failed for {model_path} on {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Store results for final summary\n",
        "results = []\n",
        "\n",
        "# Run inference on all images using both models\n",
        "for model in models:\n",
        "    print(f\"\\nRunning inference with model: {model}\\n\" + \"=\"*60)\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(image_folder, image_file)\n",
        "        result = run_inference(model, image_path)\n",
        "        if result:\n",
        "            results.append(result)\n",
        "\n",
        "# Final Summary Table\n",
        "shutil.get_terminal_size((80, 20))  # Ensure table formatting works well\n",
        "print(\"\\nFinal Inference Summary\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Model':<40} {'Image':<20} {'Time (s)':<10} {'Pred Class':<12} {'CPU (%)':<12} {'Mem (%)':<10} {'Temp (°C)':<10}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for res in results:\n",
        "    print(f\"{res['model']:<40} {res['image']:<20} {res['inference_time']:<10} {res['predicted_class']:<12} {res['cpu_after']:<12} {res['mem_after']:<10} {res['temp_after']:<10}\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Inference Completed.\")\n"
      ],
      "metadata": {
        "id": "0FzTsLQDOqS4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "237b883d-81fd-4d64-ec15-63655cfbbb98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tflite_runtime'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-03cb71981a8f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtflite_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpreter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tflite_runtime'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Inference Results and answer the questions (10 marks)\n",
        "## **Attach screenshots of your raspberry pi outputs**\n",
        "\n",
        "### Instructions:\n",
        "- Review the summary table generated after running the inference.\n",
        "- Compare the performance of the 2 different models and analyze the resource usage.\n",
        "\n",
        "### Key Observations:\n",
        "1. **Inference Time:** Compare the execution times for different models. Which model runs faster? How does quantization affect speed?\n",
        "2. **CPU and Memory Usage:** Observe CPU and memory utilization before and after inference. Do different models show significant differences in resource consumption?\n",
        "3. **Prediction Consistency:** Look at the predicted classes. Are the models making consistent predictions across different images?\n"
      ],
      "metadata": {
        "id": "PZ9RdDttPn4N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gs9LPxDtLZjn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}